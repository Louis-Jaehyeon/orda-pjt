#!/usr/bin/env python3
"""
RAG ë¶„ì„ ì‹œìŠ¤í…œ ëª¨ë“ˆ - FastAPI ì—°ë™ìš©
í˜„ì¬ ë‰´ìŠ¤ â†’ ê³¼ê±° ì´ìŠˆ ë§¤ì¹­ â†’ ì‚°ì—… ì—°ê²° â†’ ì¢…í•© ë¶„ì„
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import traceback

from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

class RAGAnalyzer:
    """RAG ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™” ë° í™˜ê²½ ì„¤ì •"""
        load_dotenv(override=True)
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        
        if not self.openai_api_key or not self.pinecone_api_key:
            raise ValueError("OPENAI_API_KEY ë˜ëŠ” PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o", 
            temperature=0, 
            max_tokens=4095,
            api_key=self.openai_api_key
        )
        
        self.embedding = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=self.openai_api_key
        )
        
        # Pinecone ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self._init_vector_stores()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self._init_prompts()
    
    def _init_vector_stores(self):
        """Pinecone ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            # ì‚°ì—… ë¶„ë¥˜ ë²¡í„° ìŠ¤í† ì–´
            self.industry_store = PineconeVectorStore(
                index_name="lastproject",
                embedding=self.embedding,
                namespace="industry"
            )
            
            # ê³¼ê±° ì´ìŠˆ ë²¡í„° ìŠ¤í† ì–´  
            self.past_issue_store = PineconeVectorStore(
                index_name="lastproject",
                embedding=self.embedding,
                namespace="past_issue"
            )
            
            print("âœ… Pinecone ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _init_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        
        # ì¢…í•© ë¶„ì„ í”„ë¡¬í”„íŠ¸
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """
ë‹¹ì‹ ì€ ì£¼ì‹ ì‹œì¥ì— ëŒ€í•œ í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ê°œì¸ íˆ¬ììë“¤ì´ ì‹œì¥ ë‰´ìŠ¤ë¥¼ ì´í•´í•˜ê³  í˜„ëª…í•œ íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- í˜„ì¬ ë‰´ìŠ¤ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê³ ,
- ê³¼ê±° ìœ ì‚¬ ë‰´ìŠ¤ì™€ ë¹„êµí•´ ë³€í™”ëœ ì ì„ ë¶„ì„í•˜ë©°,  
- ê´€ë ¨ ì‚°ì—…ì˜ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ ì˜í–¥ì´ ìˆì„ ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤.

ì„¤ëª…ì€ íˆ¬ìì ì…ì¥ì—ì„œ ì•Œê¸° ì‰½ê²Œ í’€ì–´ì£¼ê³ , ì „ë¬¸ ìš©ì–´ëŠ” í”¼í•˜ë©°, ì‹¤ì œ ì‚¬ë¡€ë‚˜ ë¹„ìœ ë¥¼ í™œìš©í•´ ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤.
íˆ¬ì íŒë‹¨ì— ì°¸ê³ í•  ë§Œí•œ ì‹œì‚¬ì ì´ë‚˜ ì£¼ì˜í•  ì ë„ í¬í•¨í•´ ì£¼ì„¸ìš”.

**ì¤‘ìš”**: íˆ¬ì ì¶”ì²œì´ë‚˜ ë§¤ìˆ˜/ë§¤ë„ ê¶Œìœ ëŠ” ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”. êµìœ¡ì  ë¶„ì„ë§Œ ì œê³µí•˜ì„¸ìš”.
            """),
            ("human", """
ì•„ë˜ ì„¸ ê°€ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.

## í˜„ì¬ ë‰´ìŠ¤:
{current_news}

## ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ:
{past_issues}

## ê´€ë ¨ ì‚°ì—… ì •ë³´:
{industry_context}

í˜•ì‹: 2~3ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ, ëª…í™•í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.  
ë¶„ì„ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            """)
        ])
        
        # ì‹ ë¢°ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸
        self.confidence_prompt = ChatPromptTemplate.from_messages([
            ("system", """
ë‹¹ì‹ ì€ ë¶„ì„ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜„ì¬ ë‰´ìŠ¤ì™€ ê³¼ê±° ì´ìŠˆ, ì‚°ì—… ì •ë³´ ê°„ì˜ ê´€ë ¨ì„±ì„ 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- 0.9~1.0: ë§¤ìš° ë†’ì€ ìœ ì‚¬ì„±, ëª…í™•í•œ ì—°ê´€ì„±
- 0.7~0.8: ë†’ì€ ìœ ì‚¬ì„±, ê°•í•œ ì—°ê´€ì„±  
- 0.5~0.6: ë³´í†µ ìœ ì‚¬ì„±, ì ë‹¹í•œ ì—°ê´€ì„±
- 0.3~0.4: ë‚®ì€ ìœ ì‚¬ì„±, ì•½í•œ ì—°ê´€ì„±
- 0.0~0.2: ë§¤ìš° ë‚®ì€ ìœ ì‚¬ì„±, ê±°ì˜ ë¬´ê´€

JSON í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”: {"confidence": ì ìˆ˜, "reason": "í‰ê°€ ì´ìœ "}
            """),
            ("human", """
í˜„ì¬ ë‰´ìŠ¤: {current_news}
ê³¼ê±° ì´ìŠˆ: {past_issues}
ê´€ë ¨ ì‚°ì—…: {industry_context}
            """)
        ])
    
    async def search_similar_past_issues(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰"""
        try:
            print(f"ğŸ” ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰: '{query[:50]}...'")
            
            # Pineconeì—ì„œ ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰
            similar_docs = self.past_issue_store.similarity_search(
                query, 
                k=top_k,
                namespace="past_issue"
            )
            
            results = []
            for i, doc in enumerate(similar_docs):
                # ë¬¸ì„œ ë‚´ìš© íŒŒì‹±
                content_lines = doc.page_content.split('\n')
                
                issue_data = {
                    "rank": i + 1,
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "parsed_info": self._parse_past_issue_content(content_lines)
                }
                
                results.append(issue_data)
            
            print(f"âœ… {len(results)}ê°œ ìœ ì‚¬ ì´ìŠˆ ë°œê²¬")
            return results
            
        except Exception as e:
            print(f"âŒ ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def search_related_industries(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰"""
        try:
            print(f"ğŸ­ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰: '{query[:50]}...'")
            
            # Pineconeì—ì„œ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰
            similar_docs = self.industry_store.similarity_search(
                query,
                k=top_k, 
                namespace="industry"
            )
            
            results = []
            for i, doc in enumerate(similar_docs):
                content_lines = doc.page_content.split('\n')
                
                industry_data = {
                    "rank": i + 1,
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "parsed_info": self._parse_industry_content(content_lines)
                }
                
                results.append(industry_data)
            
            print(f"âœ… {len(results)}ê°œ ê´€ë ¨ ì‚°ì—… ë°œê²¬")
            return results
            
        except Exception as e:
            print(f"âŒ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_past_issue_content(self, content_lines: List[str]) -> Dict[str, str]:
        """ê³¼ê±° ì´ìŠˆ ë‚´ìš© íŒŒì‹±"""
        parsed = {
            "issue_name": "ë¯¸í™•ì¸",
            "related_industries": "ë¯¸í™•ì¸", 
            "industry_reason": "ë¯¸í™•ì¸",
            "start_date": "ë¯¸í™•ì¸",
            "end_date": "ë¯¸í™•ì¸"
        }
        
        for line in content_lines:
            line = line.strip()
            if line.startswith('Issue_name:'):
                parsed["issue_name"] = line.replace('Issue_name:', '').strip()
            elif line.startswith('ê´€ë ¨ ì‚°ì—…:'):
                parsed["related_industries"] = line.replace('ê´€ë ¨ ì‚°ì—…:', '').strip()
            elif line.startswith('ì‚°ì—… ì´ìœ :'):
                parsed["industry_reason"] = line.replace('ì‚°ì—… ì´ìœ :', '').strip()
            elif line.startswith('Start_date:'):
                parsed["start_date"] = line.replace('Start_date:', '').strip()
            elif line.startswith('Fin_date:'):
                parsed["end_date"] = line.replace('Fin_date:', '').strip()
        
        return parsed
    
    def _parse_industry_content(self, content_lines: List[str]) -> Dict[str, str]:
        """ì‚°ì—… ì •ë³´ ë‚´ìš© íŒŒì‹±"""
        parsed = {
            "industry_name": "ë¯¸í™•ì¸",
            "description": ""
        }
        
        for line in content_lines:
            line = line.strip()
            if line.startswith('KRX ì—…ì¢…ëª…:') or line.startswith('ï»¿KRX ì—…ì¢…ëª…:'):
                parsed["industry_name"] = line.replace('KRX ì—…ì¢…ëª…:', '').replace('ï»¿KRX ì—…ì¢…ëª…:', '').strip()
            elif line.startswith('ìƒì„¸ë‚´ìš©:'):
                parsed["description"] = line.replace('ìƒì„¸ë‚´ìš©:', '').strip()
            elif parsed["description"] and not line.startswith('KRX') and line:
                # ìƒì„¸ë‚´ìš© continuation
                parsed["description"] += " " + line
        
        return parsed
    
    async def comprehensive_analysis(
        self, 
        current_news: str, 
        past_issues: Optional[List[Dict]] = None,
        industries: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ì‹¤í–‰"""
        try:
            print("ğŸ§  ì¢…í•© ë¶„ì„ ì‹œì‘...")
            
            # 1. ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰ (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
            if past_issues is None:
                past_issues = await self.search_similar_past_issues(current_news)
            
            # 2. ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰ (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)  
            if industries is None:
                industries = await self.search_related_industries(current_news)
            
            # 3. ë¶„ì„ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„
            past_issues_text = self._format_past_issues_for_analysis(past_issues)
            industry_text = self._format_industries_for_analysis(industries)
            
            # 4. LLM ì¢…í•© ë¶„ì„
            analysis_chain = self.analysis_prompt | self.llm | StrOutputParser()
            
            explanation = await asyncio.to_thread(
                analysis_chain.invoke,
                {
                    "current_news": current_news,
                    "past_issues": past_issues_text,
                    "industry_context": industry_text
                }
            )
            
            # 5. ì‹ ë¢°ë„ í‰ê°€
            confidence_chain = self.confidence_prompt | self.llm | JsonOutputParser()
            
            confidence_result = await asyncio.to_thread(
                confidence_chain.invoke,
                {
                    "current_news": current_news,
                    "past_issues": past_issues_text,
                    "industry_context": industry_text
                }
            )
            
            result = {
                "explanation": explanation,
                "confidence": confidence_result.get("confidence", 0.5),
                "confidence_reason": confidence_result.get("reason", "í‰ê°€ ë¶ˆê°€"),
                "past_issues_used": len(past_issues),
                "industries_used": len(industries),
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            print(f"âœ… ì¢…í•© ë¶„ì„ ì™„ë£Œ (ì‹ ë¢°ë„: {result['confidence']:.2f})")
            return result
            
        except Exception as e:
            print(f"âŒ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            
            # í´ë°± ì‘ë‹µ
            return {
                "explanation": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "confidence": 0.0,
                "confidence_reason": f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}",
                "past_issues_used": 0,
                "industries_used": 0,
                "analysis_timestamp": datetime.now().isoformat()
            }
    
    def _format_past_issues_for_analysis(self, past_issues: List[Dict]) -> str:
        """ê³¼ê±° ì´ìŠˆë“¤ì„ ë¶„ì„ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not past_issues:
            return "ê´€ë ¨ëœ ê³¼ê±° ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_texts = []
        for i, issue in enumerate(past_issues[:3]):  # ìµœëŒ€ 3ê°œë§Œ ì‚¬ìš©
            parsed = issue.get("parsed_info", {})
            
            text = f"""
[ê³¼ê±° ì´ìŠˆ {i+1}]
â€¢ ì´ìŠˆëª…: {parsed.get('issue_name', 'ë¯¸í™•ì¸')}
â€¢ ê´€ë ¨ ì‚°ì—…: {parsed.get('related_industries', 'ë¯¸í™•ì¸')}
â€¢ ì‚°ì—… ì˜í–¥ ì´ìœ : {parsed.get('industry_reason', 'ë¯¸í™•ì¸')}
â€¢ ë°œìƒ ê¸°ê°„: {parsed.get('start_date', 'ë¯¸í™•ì¸')} ~ {parsed.get('end_date', 'ë¯¸í™•ì¸')}
            """.strip()
            
            formatted_texts.append(text)
        
        return "\n\n".join(formatted_texts)
    
    def _format_industries_for_analysis(self, industries: List[Dict]) -> str:
        """ê´€ë ¨ ì‚°ì—…ë“¤ì„ ë¶„ì„ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not industries:
            return "ê´€ë ¨ëœ ì‚°ì—… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_texts = []
        for i, industry in enumerate(industries[:3]):  # ìµœëŒ€ 3ê°œë§Œ ì‚¬ìš©
            parsed = industry.get("parsed_info", {})
            
            text = f"""
[ê´€ë ¨ ì‚°ì—… {i+1}]
â€¢ ì‚°ì—…ëª…: {parsed.get('industry_name', 'ë¯¸í™•ì¸')}  
â€¢ ìƒì„¸ ì„¤ëª…: {parsed.get('description', 'ì„¤ëª… ì—†ìŒ')[:200]}...
            """.strip()
            
            formatted_texts.append(text)
        
        return "\n\n".join(formatted_texts)
    
    async def quick_analysis(self, news_content: str) -> Dict[str, Any]:
        """ë¹ ë¥¸ ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            # ê¸°ë³¸ ì‚°ì—… ë§¤ì¹­ë§Œ ìˆ˜í–‰
            industries = await self.search_related_industries(news_content, top_k=2)
            
            if not industries:
                return {
                    "explanation": "ê´€ë ¨ ì‚°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "confidence": 0.0,
                    "industries": []
                }
            
            # ê°„ë‹¨í•œ ì„¤ëª… ìƒì„±
            industry_names = [ind.get("parsed_info", {}).get("industry_name", "ë¯¸í™•ì¸") 
                            for ind in industries]
            
            explanation = f"""
í˜„ì¬ ë‰´ìŠ¤ëŠ” ì£¼ë¡œ {', '.join(industry_names)} ì‚°ì—…ê³¼ ê´€ë ¨ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì‚°ì—…ë“¤ì€ í•´ë‹¹ ë‰´ìŠ¤ì˜ ì˜í–¥ì„ ì§ê°„ì ‘ì ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, 
íˆ¬ììë“¤ì€ ì´ëŸ¬í•œ ì‚°ì—… ë™í–¥ì„ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ë” ìƒì„¸í•œ ë¶„ì„ì„ ì›í•˜ì‹œë©´ ì „ì²´ ë¶„ì„ ê¸°ëŠ¥ì„ ì´ìš©í•´ì£¼ì„¸ìš”.
            """.strip()
            
            return {
                "explanation": explanation,
                "confidence": 0.6,
                "industries": industry_names
            }
            
        except Exception as e:
            print(f"âŒ ë¹ ë¥¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "explanation": "ë¹ ë¥¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "confidence": 0.0,
                "industries": []
            }

# ===== í…ŒìŠ¤íŠ¸ ë° ì§ì ‘ ì‹¤í–‰ìš© =====

async def test_rag_system():
    """RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª RAG ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    try:
        # RAG ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = RAGAnalyzer()
        
        # í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤
        test_news = """
SKí…”ë ˆì½¤(SKT)ì´ ìµœê·¼ ì‚¬ì´ë²„ ë³´ì•ˆ ì‚¬ê³ ë¡œ ì¸í•´ ê³ ê°ë“¤ì—ê²Œ ê³„ì•½ í•´ì§€ ìˆ˜ìˆ˜ë£Œë¥¼ ë©´ì œí•´ ì£¼ê² ë‹¤ê³  ë°œí‘œí–ˆëŠ”ë°, 
ì´ë¡œ ì¸í•´ ë§ì€ ê³ ê°ë“¤ì´ ê²½ìŸì‚¬ì¸ KTì™€ LG U+ë¡œ ì´ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
í•˜ë£¨ì—ë§Œ 6,656ëª…ì˜ ìˆœì†ì‹¤ì„ ê¸°ë¡í–ˆìœ¼ë©°, ì¼ì£¼ì¼ ë™ì•ˆ 28,566ëª…ì˜ ìˆœì†ì‹¤ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
        """
        
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤: {test_news[:100]}...")
        
        # 1. ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        past_issues = await analyzer.search_similar_past_issues(test_news)
        for issue in past_issues:
            parsed = issue["parsed_info"]
            print(f"  â€¢ {parsed['issue_name']}")
        
        # 2. ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸  
        print("\n2ï¸âƒ£ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        industries = await analyzer.search_related_industries(test_news)
        for industry in industries:
            parsed = industry["parsed_info"]
            print(f"  â€¢ {parsed['industry_name']}")
        
        # 3. ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸")
        analysis = await analyzer.comprehensive_analysis(test_news)
        print(f"ì‹ ë¢°ë„: {analysis['confidence']:.2f}")
        print(f"ë¶„ì„ ê²°ê³¼:\n{analysis['explanation'][:200]}...")
        
        # 4. ë¹ ë¥¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
        print("\n4ï¸âƒ£ ë¹ ë¥¸ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        quick = await analyzer.quick_analysis(test_news)
        print(f"ê´€ë ¨ ì‚°ì—…: {quick['industries']}")
        
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    asyncio.run(test_rag_system())